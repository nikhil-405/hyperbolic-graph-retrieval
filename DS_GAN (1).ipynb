{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "Tjoo1Z9lnsxl"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "amJ2JPRAqAY9",
        "outputId": "77657dac-a80f-49ba-bb36-db08bcc812a1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: scipy==1.10.1 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (1.10.1)\n",
            "Requirement already satisfied: numpy<1.27.0,>=1.19.5 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from scipy==1.10.1) (1.26.4)\n"
          ]
        }
      ],
      "source": [
        "!pip install scipy==1.10.1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zYdBfId1qHte",
        "outputId": "3ef05168-693f-4323-e1ba-7627f885d402"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1.10.1\n"
          ]
        }
      ],
      "source": [
        "import scipy\n",
        "print(scipy.__version__)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "IroHqGqUrBMv"
      },
      "outputs": [],
      "source": [
        "# All Losses Together\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "class MyHashLoss(nn.Module):\n",
        "    def __init__(self, q_wt=0.1, adv_wt=0.01, kern_wt=0.1):\n",
        "        super(MyHashLoss, self).__init__()\n",
        "        self.q_wt = q_wt        # weight for quant loss\n",
        "        self.adv_wt = adv_wt    # weight for adv loss\n",
        "        self.kern_wt = kern_wt  # (not used now but maybe later)\n",
        "\n",
        "    def quant_loss(self, h):\n",
        "        # want values close to -1 or 1, so abs - 1 should be small\n",
        "        return torch.mean((h.abs() - 1) ** 2)\n",
        "\n",
        "    def sim_loss(self, real_sim, hash_sim):\n",
        "        # check how diff the sim matrices are\n",
        "        return torch.mean((real_sim - hash_sim) ** 2)\n",
        "\n",
        "    def kernel_loss(self, pred_kernel, true_kernel):\n",
        "        # maybe use later - align kernel with GT\n",
        "        return torch.mean((pred_kernel - true_kernel) ** 2)\n",
        "\n",
        "    def forward(self, out, gt_sim, adv):\n",
        "        cont = out['continuous_hash']\n",
        "        bin_hash = out['binary_hash']  # not used now\n",
        "        kern_sim = out['kernel_similarity']\n",
        "\n",
        "        q_loss = self.quant_loss(cont)\n",
        "        s_loss = self.sim_loss(gt_sim, kern_sim)\n",
        "\n",
        "        total = s_loss + self.q_wt * q_loss + self.adv_wt * adv\n",
        "\n",
        "        return total, {\n",
        "            'quant_loss': q_loss.item(),\n",
        "            'sim_loss': s_loss.item(),\n",
        "            'adv_loss': adv.item(),\n",
        "            'total_loss': total.item()\n",
        "        }\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YdtSYdcrxpGX",
        "outputId": "b00d3bb7-f32e-48df-99d7-7140765f7011"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: geoopt in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (0.5.0)\n",
            "Requirement already satisfied: torch>=1.9.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from geoopt) (2.2.1+cu121)\n",
            "Requirement already satisfied: numpy in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from geoopt) (1.26.4)\n",
            "Requirement already satisfied: scipy in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from geoopt) (1.10.1)\n",
            "Requirement already satisfied: filelock in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from torch>=1.9.0->geoopt) (3.18.0)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from torch>=1.9.0->geoopt) (4.13.2)\n",
            "Requirement already satisfied: sympy in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from torch>=1.9.0->geoopt) (1.13.3)\n",
            "Requirement already satisfied: networkx in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from torch>=1.9.0->geoopt) (3.2.1)\n",
            "Requirement already satisfied: jinja2 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from torch>=1.9.0->geoopt) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from torch>=1.9.0->geoopt) (2024.12.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from torch>=1.9.0->geoopt) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from torch>=1.9.0->geoopt) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from torch>=1.9.0->geoopt) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from torch>=1.9.0->geoopt) (8.9.2.26)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from torch>=1.9.0->geoopt) (12.1.3.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from torch>=1.9.0->geoopt) (11.0.2.54)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from torch>=1.9.0->geoopt) (10.3.2.106)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from torch>=1.9.0->geoopt) (11.4.5.107)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from torch>=1.9.0->geoopt) (12.1.0.106)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.19.3 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from torch>=1.9.0->geoopt) (2.19.3)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from torch>=1.9.0->geoopt) (12.1.105)\n",
            "Requirement already satisfied: triton==2.2.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from torch>=1.9.0->geoopt) (2.2.0)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from nvidia-cusolver-cu12==11.4.5.107->torch>=1.9.0->geoopt) (12.8.93)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from jinja2->torch>=1.9.0->geoopt) (3.0.2)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from sympy->torch>=1.9.0->geoopt) (1.3.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install geoopt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "DchK1c2hoCZU"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import geoopt\n",
        "\n",
        "import torch\n",
        "import geoopt\n",
        "\n",
        "# Function to get the average point (Fr√©chet mean) on Lorentz manifold\n",
        "def lorentz_mean(manifold, points, max_iter=10, tol=1e-5):\n",
        "    \"\"\"\n",
        "    Finds average point (kind of like center) for given points on Lorentz manifold\n",
        "    points: [N, D] shaped tensor\n",
        "    \"\"\"\n",
        "    dev = points.device  # use same device as input\n",
        "    mu = points[0].to(dev)  # just take the first point as starting point\n",
        "\n",
        "    for _ in range(max_iter):\n",
        "        tangents = manifold.logmap(mu, points)  # get tangent vectors from mu to all points\n",
        "        avg_tangent = tangents.mean(dim=0)      # average of those tangents\n",
        "        if avg_tangent.norm().item() < tol:     # stop if changes are very small\n",
        "            break\n",
        "        mu = manifold.expmap(mu, avg_tangent)   # move mu a bit in direction of avg_tangent\n",
        "\n",
        "    return mu\n",
        "\n",
        "\n",
        "import torch\n",
        "import geoopt\n",
        "\n",
        "# Basic k-means but for hyperbolic space using Lorentz model\n",
        "def hyperbolic_kmeans(x, k, manifold, max_iter=20):\n",
        "    \"\"\"\n",
        "    Does k-means in hyperbolic space\n",
        "    x: [B, D] points on manifold\n",
        "    k: number of clusters\n",
        "    \"\"\"\n",
        "    B, D = x.shape\n",
        "    perm = torch.randperm(B)  # shuffle indices\n",
        "    centroids = x[perm[:k]].clone()  # pick k random points as starting centroids\n",
        "\n",
        "    for _ in range(max_iter):\n",
        "        # calculate distance from every x to each centroid\n",
        "        dists = manifold.dist(\n",
        "            x.unsqueeze(1),           # [B, 1, D]\n",
        "            centroids.unsqueeze(0)    # [1, k, D]\n",
        "        ).squeeze(1)                  # final shape: [B, k]\n",
        "\n",
        "        labels = dists.argmin(dim=1)  # assign each x to nearest centroid\n",
        "\n",
        "        new_centroids = []\n",
        "        for ci in range(k):\n",
        "            pts = x[labels == ci]  # all points belonging to cluster ci\n",
        "            if pts.numel() == 0:\n",
        "                # if no point got assigned, just pick random one again\n",
        "                rand_idx = torch.randint(0, B, (1,)).item()\n",
        "                new_centroids.append(x[rand_idx])\n",
        "            else:\n",
        "                mu = lorentz_mean(manifold, pts)  # get average point\n",
        "                new_centroids.append(mu)\n",
        "\n",
        "        centroids = torch.stack(new_centroids, dim=0)\n",
        "\n",
        "    return centroids, labels  # return final cluster centers + which point goes where\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "Mi3kPzhjrgqO"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import geoopt\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import geoopt\n",
        "\n",
        "# Generator model that gives both continuous and binary hashes\n",
        "class HashGen(nn.Module):\n",
        "    def __init__(self, in_dim, hid_dim, out_dim, manifold=None):\n",
        "        \"\"\"\n",
        "        in_dim  : input embedding size (from HypFormer maybe)\n",
        "        hid_dim : size of middle layer\n",
        "        out_dim : size of hash code (final output)\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "        self.manifold = manifold or geoopt.manifolds.Lorentz()  # use Lorentz if not given\n",
        "\n",
        "        self.fc1 = nn.Linear(in_dim, hid_dim)    # input to hidden\n",
        "        self.fc_cont = nn.Linear(hid_dim, out_dim)  # for continuous hash\n",
        "        self.fc_bin = nn.Linear(hid_dim, out_dim)   # for binary hash\n",
        "\n",
        "    def forward(self, x):\n",
        "        dev = x.device  # stick to same device\n",
        "\n",
        "        # Step 1: take input from manifold to flat space\n",
        "        x_tan = self.manifold.logmap0(x)\n",
        "\n",
        "        # Step 2: pass through hidden layer\n",
        "        h = F.relu(self.fc1(x_tan))\n",
        "\n",
        "        # Step 3: one branch gives continuous hash\n",
        "        cont_out = self.fc_cont(h)\n",
        "        cont_hash = self.manifold.expmap0(cont_out)  # back to curved space\n",
        "\n",
        "        # Step 4: other branch gives binary hash\n",
        "        bin_out = self.fc_bin(h)\n",
        "        bin_hash = torch.sign(bin_out)  # just hard sign\n",
        "\n",
        "        return cont_hash.to(dev), bin_hash.to(dev)  # return both\n",
        "\n",
        "\n",
        "\n",
        "# Simple Discriminator for GAN training\n",
        "class Discrim(nn.Module):\n",
        "    def __init__(self, hash_dim=64, hid_dim=128, manifold=None):\n",
        "        \"\"\"\n",
        "        hash_dim : size of the input hash (should match generator output)\n",
        "        hid_dim  : hidden layer size\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "        self.manifold = manifold or geoopt.manifolds.Lorentz()\n",
        "\n",
        "        self.fc1 = nn.Linear(hash_dim, hid_dim)\n",
        "        self.fc2 = nn.Linear(hid_dim, 1)  # final binary output\n",
        "\n",
        "    def forward(self, h):\n",
        "        dev = h.device\n",
        "\n",
        "        # Step 1: take from manifold to flat space\n",
        "        h_tan = self.manifold.logmap0(h)\n",
        "\n",
        "        # Step 2: normal 2-layer MLP\n",
        "        x = F.relu(self.fc1(h_tan))\n",
        "        out = self.fc2(x)\n",
        "        real_score = torch.sigmoid(out).view(-1)  # squish to [0,1]\n",
        "\n",
        "        return real_score.to(dev)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "3pjWpBlxl3ga"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import geoopt\n",
        "\n",
        "# Trainer class to handle GAN-style training for hyperbolic hash learning\n",
        "class HashTrainer:\n",
        "    def __init__(\n",
        "        self,\n",
        "        gen: nn.Module,\n",
        "        disc: nn.Module,\n",
        "        manifold: geoopt.manifolds.Lorentz,\n",
        "        kmeans_func,\n",
        "        num_clusters: int,\n",
        "        lr: float = 2e-4,\n",
        "        beta1: float = 0.5,\n",
        "        beta2: float = 0.999,\n",
        "        noise_std: float = 0.1,\n",
        "        alpha: float = 0.7,\n",
        "    ):\n",
        "        self.gen = gen.to(\"cpu\")\n",
        "        self.disc = disc.to(\"cpu\")\n",
        "        self.manifold = manifold\n",
        "        self.kmeans_func = kmeans_func\n",
        "        self.k = num_clusters\n",
        "        self.noise_std = noise_std\n",
        "        self.alpha = alpha\n",
        "\n",
        "        self.adv_loss = nn.BCEWithLogitsLoss()\n",
        "\n",
        "        self.opt_g = torch.optim.Adam(self.gen.parameters(), lr=lr, betas=(beta1, beta2))\n",
        "        self.opt_d = torch.optim.Adam(self.disc.parameters(), lr=lr, betas=(beta1, beta2))\n",
        "\n",
        "    def lorentz_dot(self, x, y):\n",
        "        return -x[..., 0] * y[..., 0] + torch.sum(x[..., 1:] * y[..., 1:], dim=-1)\n",
        "\n",
        "    def proj_tangent(self, x, v):\n",
        "        inner = self.lorentz_dot(x, v).unsqueeze(-1)\n",
        "        return v + inner * x\n",
        "\n",
        "    def train_step(self, embeds: torch.Tensor):\n",
        "        embeds = embeds.to(\"cpu\")\n",
        "        embeds = self.manifold.projx(embeds)\n",
        "\n",
        "        B = embeds.shape[0]\n",
        "        real_label = torch.ones(B)\n",
        "        fake_label = torch.zeros(B)\n",
        "\n",
        "        # Get pseudo-labels using k-means in hyperbolic space\n",
        "        with torch.no_grad():\n",
        "            cents, pseudo = self.kmeans_func(embeds, k=self.k)\n",
        "            cents = self.manifold.projx(cents)\n",
        "\n",
        "        # Run Generator to get both types of hash\n",
        "        cont_raw, bin_hash = self.gen(embeds)\n",
        "        cont_hash = self.manifold.projx(cont_raw)\n",
        "\n",
        "        # Cluster loss: make points closer to their cluster centers\n",
        "        dists = self.manifold.dist(\n",
        "            embeds.unsqueeze(1),\n",
        "            cents.unsqueeze(0),\n",
        "        ).squeeze(1)\n",
        "        c_loss = F.cross_entropy(-dists, pseudo)\n",
        "\n",
        "        # Add hyperbolic noise\n",
        "        rand_vec = torch.randn_like(embeds)\n",
        "        tan_vec = self.proj_tangent(embeds, rand_vec)\n",
        "\n",
        "        speed_sq = self.lorentz_dot(tan_vec, tan_vec).abs().clamp(min=1e-12)\n",
        "        speed = speed_sq.sqrt().unsqueeze(-1)\n",
        "\n",
        "        noise = tan_vec / speed * self.noise_std\n",
        "        noisy_embed = self.manifold.expmap(embeds, noise)\n",
        "\n",
        "        # Hash the noisy versions\n",
        "        fake_raw, _ = self.gen(noisy_embed)\n",
        "        fake_hash = self.manifold.projx(fake_raw)\n",
        "\n",
        "        # Train Discriminator\n",
        "        real_out = self.disc(cont_hash.detach())\n",
        "        fake_out = self.disc(fake_hash.detach())\n",
        "\n",
        "        loss_real = self.adv_loss(real_out, real_label)\n",
        "        loss_fake = self.adv_loss(fake_out, fake_label)\n",
        "        d_loss = 0.5 * (loss_real + loss_fake)\n",
        "\n",
        "        self.opt_d.zero_grad()\n",
        "        d_loss.backward()\n",
        "        self.opt_d.step()\n",
        "\n",
        "        # Train Generator\n",
        "        for p in self.disc.parameters():\n",
        "            p.requires_grad_(False)\n",
        "\n",
        "        gen_out = self.disc(fake_hash)\n",
        "        g_adv = self.adv_loss(gen_out, real_label)\n",
        "        g_loss = self.alpha * c_loss + (1 - self.alpha) * g_adv\n",
        "\n",
        "        self.opt_g.zero_grad()\n",
        "        g_loss.backward()\n",
        "        self.opt_g.step()\n",
        "\n",
        "        for p in self.disc.parameters():\n",
        "            p.requires_grad_(True)\n",
        "\n",
        "        return d_loss.item(), g_loss.item(), c_loss.item()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 1/50:   0%|          | 0/515 [00:00<?, ?it/s]"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 1/50:  24%|‚ñà‚ñà‚ñç       | 124/515 [01:38<05:17,  1.23it/s]"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import geoopt\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "\n",
        "device = torch.device(\"cpu\")\n",
        "\n",
        "# Load precomputed embeddings (saved as .npy file)\n",
        "raw_embeddings = np.load('hyperbolic_embeddings.npy')  # shape [N, D]\n",
        "manifold = geoopt.manifolds.Lorentz()\n",
        "embeddings = torch.from_numpy(raw_embeddings).float().to(device)\n",
        "embeddings = manifold.projx(embeddings)  # project to hyperboloid\n",
        "\n",
        "# Model stuff\n",
        "B, input_dim = embeddings.shape\n",
        "hidden_dim = 512\n",
        "hash_dim = 64\n",
        "num_classes = 10\n",
        "batch_size = 64\n",
        "epochs = 50\n",
        "\n",
        "# Models\n",
        "gen = HashGen(input_dim, hidden_dim, hash_dim, manifold).to(device)\n",
        "disc = Discrim(hash_dim).to(device)\n",
        "\n",
        "# Trainer\n",
        "trainer = HashTrainer(\n",
        "    gen=gen,\n",
        "    disc=disc,\n",
        "    manifold=manifold,\n",
        "    kmeans_func=lambda x, k: hyperbolic_kmeans(x, k, manifold),\n",
        "    num_clusters=num_classes,\n",
        "    lr=2e-4,\n",
        "    noise_std=0.1,\n",
        "    alpha=0.7,\n",
        ")\n",
        "\n",
        "# Training loop\n",
        "for ep in range(epochs):\n",
        "    d_total, g_total, c_total = 0.0, 0.0, 0.0\n",
        "    steps = 0\n",
        "\n",
        "    for i in tqdm(range(0, B, batch_size), desc=f\"Epoch {ep + 1}/{epochs}\"):\n",
        "        batch = embeddings[i:i + batch_size]\n",
        "        d_loss, g_loss, c_loss = trainer.train_step(batch)\n",
        "\n",
        "        d_total += d_loss\n",
        "        g_total += g_loss\n",
        "        c_total += c_loss\n",
        "        steps += 1\n",
        "\n",
        "    avg_d = d_total / steps\n",
        "    avg_g = g_total / steps\n",
        "    avg_c = c_total / steps\n",
        "\n",
        "    print(f\"[Epoch {ep + 1}/{epochs}] D: {avg_d:.4f} | G: {avg_g:.4f} | Cluster: {avg_c:.4f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {},
      "outputs": [],
      "source": [
        "cont_hash, bin_hash = generator(embeddings)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {},
      "outputs": [],
      "source": [
        "np.save('hashes.npy', bin_hash.detach().numpy())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "r2mKzZa_B1DB"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "hashes = np.load(\"hashes.npy\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [],
      "source": [
        "emb_np = np.load('hyperbolic_embeddings.npy')          # shape [N, D]\n",
        "manifold = geoopt.manifolds.Lorentz()\n",
        "emb = torch.from_numpy(emb_np).float()\n",
        "embeddings = manifold.projx(emb)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Retrieved graph indices for query index 7:\n",
            " - Graph index: 78\n",
            " - Graph index: 53\n",
            " - Graph index: 83\n",
            " - Graph index: 37\n",
            " - Graph index: 34\n"
          ]
        }
      ],
      "source": [
        "def hamming_distance(a, b):\n",
        "    return np.sum(a != b)\n",
        "\n",
        "def lorentz_inner_product(x, y):\n",
        "    return -x[0] * y[0] + np.dot(x[1:], y[1:])\n",
        "\n",
        "def lorentz_distance(x, y):\n",
        "    inner = -lorentz_inner_product(x, y)\n",
        "    inner = np.clip(inner, 1 + 1e-5, None)\n",
        "    return np.arccosh(inner)\n",
        "\n",
        "def retrieve_neighbors(embeddings, hashes, query_index, hamming_k=100, top_k=5):\n",
        "    query_embedding = embeddings[query_index]\n",
        "    query_hash = hashes[query_index]\n",
        "\n",
        "    # Step 1: Get top `hamming_k` neighbors by Hamming distance\n",
        "    hamming_ranked = [\n",
        "        (hamming_distance(query_hash, hashes[i]), i)\n",
        "        for i in range(len(hashes)) if i != query_index\n",
        "    ]\n",
        "    hamming_ranked.sort(key=lambda x: x[0])\n",
        "    top_candidates = [i for _, i in hamming_ranked[:hamming_k]]\n",
        "\n",
        "    # Step 2: Re-rank using Lorentzian distance\n",
        "    lorentz_ranked = [\n",
        "        (lorentz_distance(query_embedding, embeddings[i]), i)\n",
        "        for i in top_candidates\n",
        "    ]\n",
        "    lorentz_ranked.sort(key=lambda x: x[0])\n",
        "\n",
        "    return [i for _, i in lorentz_ranked[:top_k]]\n",
        "\n",
        "# Example\n",
        "query_idx = 7\n",
        "retrieved_indices = retrieve_neighbors(embeddings, hashes, query_idx)\n",
        "print(f\"Retrieved graph indices for query {query_idx}:\")\n",
        "for idx in retrieved_indices:\n",
        "    print(f\" - Graph index: {idx}\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
